# 활용 가이드: 네이버 맵 크롤러

> 오프라인 리플릿 마케팅을 위한 경기도 매장 리스트 수집 실전 가이드

---

## 목차

1. [시작 전 준비](#1-시작-전-준비)
2. [기본 사용법](#2-기본-사용법)
3. [마케팅 타겟 리스트 만들기](#3-마케팅-타겟-리스트-만들기)
4. [수집 결과 Excel 활용법](#4-수집-결과-excel-활용법)
5. [지역 확장 방법](#5-지역-확장-방법)
6. [자주 발생하는 문제](#6-자주-발생하는-문제)
7. [수집 주기 및 운영 팁](#7-수집-주기-및-운영-팁)

---

## 1. 시작 전 준비

### 가상환경 활성화

매번 실행 전에 가상환경을 먼저 켜야 합니다.

```bash
# 프로젝트 폴더로 이동
cd /home/developer/mapper

# 가상환경 활성화
source venv/bin/activate
```

활성화되면 터미널 앞에 `(venv)` 표시가 나타납니다.

### 정상 동작 확인

```bash
python crawler.py --help
```

아래와 같이 출력되면 준비 완료:
```
Usage: crawler.py [OPTIONS] COMMAND [ARGS]...
  네이버 지도 크롤러 — 영업시간 포함 장소 정보 수집기
Commands:
  search  단일 검색어로 장소 정보 수집
  batch   파일의 검색어 목록을 순서대로 수집 후 단일 CSV 저장
  nearby  좌표 기반 주변 장소 수집
```

---

## 2. 기본 사용법

### 빠른 테스트 (5개만 수집)

처음 시작할 때 소량으로 테스트해 보세요.

```bash
python crawler.py search "수원시청 카페" --max 5 --output output/test.csv
```

약 30초 후 `output/test.csv` 파일이 생성됩니다.

### 단일 지역 수집 (최대 30개)

```bash
python crawler.py search "수원시청 카페" --output output/수원_카페.csv
```

### 검색어 패턴

| 목적 | 검색어 예시 |
|------|------------|
| 특정 시청 주변 카페 | `"수원시청 카페"` |
| 특정 시청 주변 음식점 | `"성남시청 음식점"` |
| 역 주변 카페 | `"수원역 카페"` |
| 대학교 주변 | `"아주대학교 카페"` |
| 특정 업종 | `"수원시청 베이커리"` |

> **팁**: 검색어는 네이버 지도 검색창에 직접 입력하는 것과 동일하게 작동합니다.

---

## 3. 마케팅 타겟 리스트 만들기

### Step 1: regions.txt 편집

`regions.txt` 파일을 열어 수집하고 싶은 지역/업종을 입력합니다.

```
수원시청 카페
수원시청 음식점
성남시청 카페
성남시청 음식점
고양시청 카페
고양시청 음식점
용인시청 카페
용인시청 음식점
부천시청 카페
부천시청 음식점
```

> 한 줄에 검색어 하나. 빈 줄은 자동으로 무시됩니다.

### Step 2: 배치 수집 실행

```bash
python crawler.py batch regions.txt \
  --max-per-query 30 \
  --output output/경기도_매장리스트.csv
```

실행하면 진행 상황이 실시간으로 표시됩니다:
```
📋 배치 시작: 10개 쿼리  (쿼리당 최대 30개)
──────────────────────────────────────────────────
[1/10] 🔍 수원시청 카페
   → 30개 place_id 수집 완료
   [1/30] 1568480811 조회 중... ✅ 런던 수원인계점
   [2/30] 1056820662 조회 중... ✅ 카페 오티티 인계본점
   ...
✅ 149개 저장: output/경기도_매장리스트.csv
```

### Step 3: 결과 확인

수집이 끝나면 `output/경기도_매장리스트.csv` 파일을 Excel로 열어 확인합니다.

> **주의**: 파일을 Excel로 열 때 `더블클릭`이 아니라 **Excel에서 파일 열기**를 사용해야 한글이 깨지지 않습니다.
> (또는 더블클릭 후 한글이 깨지면 → 데이터 탭 → 텍스트/CSV 가져오기 → 인코딩을 `UTF-8`로 선택)

---

## 4. 수집 결과 Excel 활용법

### CSV 파일 구조

| 컬럼 | 활용 방법 |
|------|----------|
| `name` | 상호명 — 리플릿 배포 대상 확인 |
| `address` | 도로명주소 — 방문 경로 파악, 지역 필터링 |
| `phone` | 전화번호 — 사전 컨택 (수집 안 된 경우도 있음) |
| `category` | 업종 — 카페/음식점 분리 필터링 |
| `business_hours` | 영업시간 — 방문 시간대 계획 |
| `visitor_reviews` | 방문자 리뷰 수 — 매장 규모/인기도 판단 |
| `blog_reviews` | 블로그 리뷰 수 — 온라인 노출도 판단 |
| `naver_place_id` | 플레이스 ID — `https://map.naver.com/p/entry/place/{ID}` |

### 추천 Excel 활용 시나리오

**시나리오 1: 리플릿 배포 우선순위 설정**
1. `visitor_reviews` 기준 내림차순 정렬
2. 방문자 리뷰 100 이상인 매장만 필터
3. 해당 매장 목록을 배포 1순위로 설정

**시나리오 2: 업종별 분리**
1. `category` 컬럼 필터 적용
2. "카페" 포함 → 카페 전용 리플릿 대상
3. "한식", "중식" 등 → 음식점 전용 리플릿 대상

**시나리오 3: 영업시간 기반 방문 계획**
1. `business_hours` 컬럼에서 "브레이크타임", "라스트오더" 시간 확인
2. 오전 방문 가능 매장 / 저녁 방문 가능 매장 분리
3. 방문 담당자별로 시간대 배분

**시나리오 4: 네이버 플레이스 직접 확인**
- `naver_place_id` 컬럼의 값을 이용해 브라우저에서 직접 확인:
  ```
  https://map.naver.com/p/entry/place/1568480811
  ```

---

## 5. 지역 확장 방법

### 경기도 전체로 확장하기

`regions.txt`에 지역을 추가하면 됩니다. 경기도 주요 31개 시·군청 기준 예시:

```
수원시청 카페
수원시청 음식점
성남시청 카페
성남시청 음식점
고양시청 카페
고양시청 음식점
용인시청 카페
용인시청 음식점
부천시청 카페
부천시청 음식점
안산시청 카페
안산시청 음식점
안양시청 카페
안양시청 음식점
평택시청 카페
평택시청 음식점
화성시청 카페
화성시청 음식점
의정부시청 카페
의정부시청 음식점
```

### 업종 세분화

```
수원시청 베이커리
수원시청 디저트카페
수원시청 브런치카페
수원시청 스터디카페
수원시청 한식
수원시청 일식
수원시청 중식
```

### 수집량 조절

```bash
# 지역당 50개 수집 (더 많은 데이터)
python crawler.py batch regions.txt --max-per-query 50 --output output/결과.csv

# 지역당 10개 수집 (빠른 테스트)
python crawler.py batch regions.txt --max-per-query 10 --output output/결과.csv
```

> **참고**: `--max-per-query` 값이 클수록 수집 시간이 늘어납니다.
> 30개 기준 쿼리 1개당 약 1~2분 소요.

---

## 6. 자주 발생하는 문제

### 문제 1: 특정 쿼리에서 0개 수집

```
⚠️  place_id를 찾을 수 없습니다: 성남시청 카페
```

**원인**: 네트워크 응답 타이밍 이슈 (간헐적 발생)
**해결**: 해당 쿼리만 단독으로 재실행

```bash
python crawler.py search "성남시청 카페" --output output/성남_카페.csv
```

### 문제 2: CSV 열었을 때 한글 깨짐

**해결**: Excel에서 직접 가져오기

1. Excel 실행 → 데이터 탭 → 텍스트/CSV 가져오기
2. 파일 선택 → 인코딩: **UTF-8** 선택
3. 구분 기호: 쉼표(,) 선택 → 완료

### 문제 3: `ModuleNotFoundError` 오류

```
ModuleNotFoundError: No module named 'playwright'
```

**원인**: 가상환경이 활성화되지 않음
**해결**:

```bash
source venv/bin/activate
python crawler.py search "수원시청 카페"
```

### 문제 4: 수집 속도가 너무 느림

배치 실행 시 쿼리당 약 1~2분이 소요됩니다. 10개 쿼리면 약 10~20분.
정상 동작이므로 터미널을 닫지 말고 기다려 주세요.

---

## 7. 수집 주기 및 운영 팁

### 권장 수집 주기

| 목적 | 권장 주기 | 이유 |
|------|-----------|------|
| 신규 매장 발굴 | 월 1회 | 신규 오픈 매장 반영 |
| 영업시간 업데이트 | 수시 | 계절/시즌 변경 반영 |
| 전체 재수집 | 분기 1회 | 폐업·이전 매장 정리 |

### IP 차단 예방

- 하루 최대 **100~200개 쿼리** 이하로 실행 권장
- 연속 실행보다 **시간 간격을 두고** 실행
- 프로그램 내 딜레이(0.5~1.0초)는 건드리지 마세요

### 결과 파일 관리

파일명에 날짜를 포함하면 이전 데이터와 비교가 쉽습니다:

```bash
python crawler.py batch regions.txt \
  --output "output/경기도_매장리스트_$(date +%Y%m%d).csv"
```

예: `경기도_매장리스트_20260220.csv`

---

## 빠른 참조

```bash
# 가상환경 켜기
source venv/bin/activate

# 단일 검색 (테스트용)
python crawler.py search "수원시청 카페" --max 5 --output output/test.csv

# 전체 배치 실행
python crawler.py batch regions.txt --max-per-query 30 --output output/경기도_매장리스트.csv

# 특정 쿼리 재실행 (0개 수집 시)
python crawler.py search "성남시청 카페" --output output/성남_카페_재시도.csv
```
