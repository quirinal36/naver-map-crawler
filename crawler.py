"""
crawler.py â€” ë„¤ì´ë²„ ë§µ í¬ë¡¤ëŸ¬ CLI ì§„ì…ì 

Usage:
  python crawler.py search "ìˆ˜ì›ì‹œì²­ ì¹´í˜" --output result.csv
  python crawler.py batch regions.txt --max-per-query 30 --output ê²½ê¸°ë„_ë§¤ì¥ë¦¬ìŠ¤íŠ¸.csv
  python crawler.py nearby --lat 37.263 --lng 127.028 --radius 500 --category cafe
"""

import asyncio
import sys
import click

from searcher import search_place_ids
from fetcher import fetch_many
from exporter import save_csv


# â”€â”€ ê³µí†µ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _collect(query: str, max_count: int) -> list[dict]:
    """ê²€ìƒ‰ì–´ í•˜ë‚˜ â†’ ìƒì„¸ì •ë³´ ëª©ë¡"""
    place_ids = await search_place_ids(query, max_count=max_count)
    if not place_ids:
        click.echo(f"   âš ï¸  place_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {query}")
        return []
    details = fetch_many(place_ids)
    click.echo(f"   â†’ {len(details)}ê°œ ìˆ˜ì§‘")
    return details


# â”€â”€ CLI ê·¸ë£¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@click.group()
def cli():
    """ë„¤ì´ë²„ ì§€ë„ í¬ë¡¤ëŸ¬ â€” ì˜ì—…ì‹œê°„ í¬í•¨ ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘ê¸°"""
    pass


# â”€â”€ search ì»¤ë§¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@cli.command()
@click.argument("query")
@click.option("--output", "-o", default="output/result.csv", show_default=True,
              help="ì €ì¥ ê²½ë¡œ")
@click.option("--max", "max_count", default=30, show_default=True,
              help="ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜")
def search(query, output, max_count):
    """ë‹¨ì¼ ê²€ìƒ‰ì–´ë¡œ ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘

    \b
    ì˜ˆì‹œ:
      python crawler.py search "ìˆ˜ì›ì‹œì²­ ì¹´í˜" --output result.csv
    """
    click.echo(f"ğŸ” ê²€ìƒ‰: {query}  (ìµœëŒ€ {max_count}ê°œ)")
    records = asyncio.run(_collect(query, max_count))
    save_csv(records, output)


# â”€â”€ batch ì»¤ë§¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@cli.command()
@click.argument("file", type=click.Path(exists=True))
@click.option("--output", "-o", default="output/result.csv", show_default=True,
              help="ì €ì¥ ê²½ë¡œ")
@click.option("--max-per-query", default=30, show_default=True,
              help="ì¿¼ë¦¬ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜")
def batch(file, output, max_per_query):
    """íŒŒì¼ì˜ ê²€ìƒ‰ì–´ ëª©ë¡ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜ì§‘ í›„ ë‹¨ì¼ CSV ì €ì¥

    \b
    ì˜ˆì‹œ:
      python crawler.py batch regions.txt --max-per-query 30 --output ê²½ê¸°ë„_ë§¤ì¥ë¦¬ìŠ¤íŠ¸.csv
    """
    with open(file, encoding="utf-8") as f:
        queries = [line.strip() for line in f if line.strip()]

    click.echo(f"ğŸ“‹ ë°°ì¹˜ ì‹œì‘: {len(queries)}ê°œ ì¿¼ë¦¬  (ì¿¼ë¦¬ë‹¹ ìµœëŒ€ {max_per_query}ê°œ)")
    click.echo("â”€" * 50)

    all_records: list[dict] = []
    for i, query in enumerate(queries, 1):
        click.echo(f"[{i}/{len(queries)}] ğŸ” {query}")
        records = asyncio.run(_collect(query, max_per_query))
        all_records.extend(records)

    click.echo("â”€" * 50)
    click.echo(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_records)}ê°œ")
    save_csv(all_records, output)


# â”€â”€ nearby ì»¤ë§¨ë“œ (P2) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@cli.command()
@click.option("--lat", required=True, type=float, help="ìœ„ë„")
@click.option("--lng", required=True, type=float, help="ê²½ë„")
@click.option("--radius", default=500, show_default=True, help="ë°˜ê²½ (ë¯¸í„°)")
@click.option("--category", default="ì¹´í˜", show_default=True, help="ì—…ì¢… í‚¤ì›Œë“œ")
@click.option("--output", "-o", default="output/nearby.csv", show_default=True,
              help="ì €ì¥ ê²½ë¡œ")
@click.option("--max", "max_count", default=30, show_default=True,
              help="ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜")
def nearby(lat, lng, radius, category, output, max_count):
    """ì¢Œí‘œ ê¸°ë°˜ ì£¼ë³€ ì¥ì†Œ ìˆ˜ì§‘ (P2)

    \b
    ì˜ˆì‹œ:
      python crawler.py nearby --lat 37.263 --lng 127.028 --radius 500 --category ì¹´í˜
    """
    # ì¢Œí‘œë¥¼ ê²€ìƒ‰ì–´ë¡œ ë³€í™˜í•´ searcherì— ì „ë‹¬ (ë„¤ì´ë²„ ì§€ë„ URL í˜•ì‹)
    # https://map.naver.com/p/search/{category}?c={lng},{lat},15,0,0,0,dh
    query = f"{category}"
    click.echo(f"ğŸ“ ì¢Œí‘œ ê¸°ë°˜ ê²€ìƒ‰: lat={lat}, lng={lng}, radius={radius}m, ì—…ì¢…={category}")
    click.echo(f"   â€» ì¢Œí‘œ ê²€ìƒ‰ì€ ê²€ìƒ‰ì–´ '{category}' ë¡œ ë™ì‘í•©ë‹ˆë‹¤ (ê°œì„  ì˜ˆì •)")
    records = asyncio.run(_collect(query, max_count))
    save_csv(records, output)


# â”€â”€ ì§„ì…ì  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    cli()
