"""
searcher.py â€” ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ì–´ â†’ place_id ëª©ë¡ ìˆ˜ì§‘
Step 1: Playwrightë¡œ ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ í›„ place_id ì¶”ì¶œ

ì „ëµ:
1. ë„¤íŠ¸ì›Œí¬ ì¸í„°ì…‰íŠ¸: API ì‘ë‹µ JSONì—ì„œ place_id íŒŒì‹±
2. DOM í´ë°±: a[href*='/place/'] íŒ¨í„´ì—ì„œ ID ì¶”ì¶œ
3. ë¬´í•œìŠ¤í¬ë¡¤: max_count ë„ë‹¬ê¹Œì§€ ë°˜ë³µ
"""

import asyncio
import re
import json
from playwright.async_api import async_playwright

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)

def _ids_from_all_search(body: dict) -> list[str]:
    """allSearch ì‘ë‹µì˜ result.place.list[].id ì§ì ‘ íŒŒì‹±"""
    ids = []
    place_list = (
        body.get("result", {}).get("place", {}).get("list", [])
        or body.get("result", {}).get("place", {}).get("filterList", [])
        or []
    )
    for item in place_list:
        pid = str(item.get("id", "")).strip()
        if pid.isdigit() and len(pid) >= 7:
            ids.append(pid)
    return ids


async def _scroll_and_collect(page, place_ids: list, max_count: int, timeout: float = 12.0):
    """ê²€ìƒ‰ ê²°ê³¼ íŒ¨ë„ ë¬´í•œìŠ¤í¬ë¡¤ë¡œ ì¶”ê°€ ê²°ê³¼ ë¡œë“œ"""
    elapsed = 0.0
    interval = 1.5
    while len(place_ids) < max_count and elapsed < timeout:
        prev = len(place_ids)
        # ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ íŒ¨ë„ ìŠ¤í¬ë¡¤ (ë„¤ì´ë²„ ì§€ë„ íŠ¹ì • ì„ íƒì)
        await page.evaluate("""
            () => {
                const panel = document.querySelector('#_pcmap_list_scroll_container')
                           || document.querySelector('.search_listview')
                           || document.querySelector('[class*="searchListView"]');
                if (panel) panel.scrollTop = panel.scrollHeight;
                else window.scrollTo(0, document.body.scrollHeight);
            }
        """)
        await asyncio.sleep(interval)
        elapsed += interval
        if len(place_ids) == prev:
            break  # ë” ì´ìƒ ìƒˆ ê²°ê³¼ ì—†ìŒ


async def _dom_fallback(page) -> list[str]:
    """DOMì—ì„œ place ë§í¬ íŒŒì‹± (í´ë°±)"""
    hrefs = await page.evaluate("""
        () => Array.from(document.querySelectorAll('a[href*="/place/"]'))
                   .map(a => a.href)
    """)
    ids = []
    for href in hrefs:
        m = re.search(r"/place/(\d{7,})", href)
        if m and m.group(1) not in ids:
            ids.append(m.group(1))
    return ids


async def search_place_ids(query: str, max_count: int = 30, headless: bool = True) -> list[str]:
    """
    ê²€ìƒ‰ì–´ë¡œ ë„¤ì´ë²„ ì§€ë„ë¥¼ ê²€ìƒ‰í•´ place_id ëª©ë¡ ë°˜í™˜

    Args:
        query: ê²€ìƒ‰ì–´ (ì˜ˆ: 'ìˆ˜ì›ì‹œì²­ ì¹´í˜')
        max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        headless: True=í—¤ë“œë¦¬ìŠ¤, False=ë¸Œë¼ìš°ì € í‘œì‹œ

    Returns:
        place_id ë¬¸ìì—´ ëª©ë¡
    """
    place_ids: list[str] = []
    seen: set[str] = set()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=headless)
        context = await browser.new_context(user_agent=USER_AGENT)
        page = await context.new_page()

        # â”€â”€ ë„¤íŠ¸ì›Œí¬ ì¸í„°ì…‰íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        async def on_response(response):
            if len(place_ids) >= max_count:
                return
            if "allSearch" not in response.url:
                return
            try:
                body = await response.json()
                for pid in _ids_from_all_search(body):
                    if pid not in seen:
                        seen.add(pid)
                        place_ids.append(pid)
            except Exception:
                pass

        page.on("response", on_response)

        # â”€â”€ ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        search_url = f"https://map.naver.com/p/search/{query}"
        print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {query}")
        print(f"   URL: {search_url}")
        try:
            await page.goto(search_url, wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(3)  # JS ë Œë”ë§ ëŒ€ê¸°
        except Exception as e:
            print(f"   [ê²½ê³ ] í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")

        # â”€â”€ ë¬´í•œìŠ¤í¬ë¡¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        await _scroll_and_collect(page, place_ids, max_count)

        # â”€â”€ DOM í´ë°± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not place_ids:
            print("   [í´ë°±] DOMì—ì„œ place_id íŒŒì‹± ì‹œë„...")
            dom_ids = await _dom_fallback(page)
            for pid in dom_ids:
                if pid not in seen:
                    seen.add(pid)
                    place_ids.append(pid)

        await browser.close()

    result = place_ids[:max_count]
    print(f"   â†’ {len(result)}ê°œ place_id ìˆ˜ì§‘ ì™„ë£Œ")
    return result


# â”€â”€ ë‹¨ë… ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import sys

    query = sys.argv[1] if len(sys.argv) > 1 else "ìˆ˜ì›ì‹œì²­ ì¹´í˜"
    max_n = int(sys.argv[2]) if len(sys.argv) > 2 else 10

    ids = asyncio.run(search_place_ids(query, max_count=max_n, headless=True))
    print("\nìˆ˜ì§‘ëœ place_id ëª©ë¡:")
    for i, pid in enumerate(ids, 1):
        print(f"  {i:2d}. {pid}  â†’  https://map.naver.com/p/entry/place/{pid}")
