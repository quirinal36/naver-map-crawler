"""
searcher.py â€” ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ì–´ â†’ place_id ëª©ë¡ ìˆ˜ì§‘
Step 1: Playwrightë¡œ ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ í›„ place_id ì¶”ì¶œ

ì „ëµ:
1. ë„¤íŠ¸ì›Œí¬ ì¸í„°ì…‰íŠ¸: API ì‘ë‹µ JSONì—ì„œ place_id íŒŒì‹±
2. DOM í´ë°±: a[href*='/place/'] íŒ¨í„´ì—ì„œ ID ì¶”ì¶œ
3. ë¬´í•œìŠ¤í¬ë¡¤: max_count ë„ë‹¬ê¹Œì§€ ë°˜ë³µ
"""

import asyncio
import re
import json
from playwright.async_api import async_playwright

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)

# place_idë¥¼ í¬í•¨í•  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì‘ë‹µ URL íŒ¨í„´
CANDIDATE_URL_PATTERNS = ["search", "place", "list", "query"]

# JSON ì‘ë‹µì—ì„œ place_idë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•˜ëŠ” í‚¤ ì´ë¦„ë“¤
PLACE_ID_KEYS = {"id", "placeId", "place_id", "businessId"}


def _extract_ids_from_obj(obj, found: set):
    """JSON ê°ì²´ë¥¼ ì¬ê·€ íƒìƒ‰í•˜ì—¬ place_id í›„ë³´ë¥¼ ìˆ˜ì§‘"""
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k in PLACE_ID_KEYS and isinstance(v, str) and v.isdigit() and len(v) >= 7:
                found.add(v)
            else:
                _extract_ids_from_obj(v, found)
    elif isinstance(obj, list):
        for item in obj:
            _extract_ids_from_obj(item, found)


async def _scroll_and_collect(page, place_ids: list, max_count: int, timeout: float = 8.0):
    """ë¬´í•œìŠ¤í¬ë¡¤ë¡œ ì¶”ê°€ ê²°ê³¼ ë¡œë“œ"""
    elapsed = 0.0
    interval = 1.5
    while len(place_ids) < max_count and elapsed < timeout:
        prev = len(place_ids)
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(interval)
        elapsed += interval
        if len(place_ids) == prev:
            break  # ë” ì´ìƒ ìƒˆ ê²°ê³¼ ì—†ìŒ


async def _dom_fallback(page) -> list[str]:
    """DOMì—ì„œ place ë§í¬ íŒŒì‹± (í´ë°±)"""
    hrefs = await page.evaluate("""
        () => Array.from(document.querySelectorAll('a[href*="/place/"]'))
                   .map(a => a.href)
    """)
    ids = []
    for href in hrefs:
        m = re.search(r"/place/(\d{7,})", href)
        if m and m.group(1) not in ids:
            ids.append(m.group(1))
    return ids


async def search_place_ids(query: str, max_count: int = 30, headless: bool = True) -> list[str]:
    """
    ê²€ìƒ‰ì–´ë¡œ ë„¤ì´ë²„ ì§€ë„ë¥¼ ê²€ìƒ‰í•´ place_id ëª©ë¡ ë°˜í™˜

    Args:
        query: ê²€ìƒ‰ì–´ (ì˜ˆ: 'ìˆ˜ì›ì‹œì²­ ì¹´í˜')
        max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        headless: True=í—¤ë“œë¦¬ìŠ¤, False=ë¸Œë¼ìš°ì € í‘œì‹œ

    Returns:
        place_id ë¬¸ìì—´ ëª©ë¡
    """
    place_ids: list[str] = []
    seen: set[str] = set()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=headless)
        context = await browser.new_context(user_agent=USER_AGENT)
        page = await context.new_page()

        # â”€â”€ ë„¤íŠ¸ì›Œí¬ ì¸í„°ì…‰íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        async def on_response(response):
            if len(place_ids) >= max_count:
                return
            url = response.url.lower()
            if not any(pat in url for pat in CANDIDATE_URL_PATTERNS):
                return
            content_type = response.headers.get("content-type", "")
            if "json" not in content_type:
                return
            try:
                body = await response.json()
                found: set[str] = set()
                _extract_ids_from_obj(body, found)
                for pid in found:
                    if pid not in seen:
                        seen.add(pid)
                        place_ids.append(pid)
            except Exception:
                pass

        page.on("response", on_response)

        # â”€â”€ ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        search_url = f"https://map.naver.com/p/search/{query}"
        print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {query}")
        print(f"   URL: {search_url}")
        try:
            await page.goto(search_url, wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(3)  # JS ë Œë”ë§ ëŒ€ê¸°
        except Exception as e:
            print(f"   [ê²½ê³ ] í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")

        # â”€â”€ ë¬´í•œìŠ¤í¬ë¡¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        await _scroll_and_collect(page, place_ids, max_count)

        # â”€â”€ DOM í´ë°± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not place_ids:
            print("   [í´ë°±] DOMì—ì„œ place_id íŒŒì‹± ì‹œë„...")
            dom_ids = await _dom_fallback(page)
            for pid in dom_ids:
                if pid not in seen:
                    seen.add(pid)
                    place_ids.append(pid)

        await browser.close()

    result = place_ids[:max_count]
    print(f"   â†’ {len(result)}ê°œ place_id ìˆ˜ì§‘ ì™„ë£Œ")
    return result


# â”€â”€ ë‹¨ë… ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import sys

    query = sys.argv[1] if len(sys.argv) > 1 else "ìˆ˜ì›ì‹œì²­ ì¹´í˜"
    max_n = int(sys.argv[2]) if len(sys.argv) > 2 else 10

    ids = asyncio.run(search_place_ids(query, max_count=max_n, headless=True))
    print("\nìˆ˜ì§‘ëœ place_id ëª©ë¡:")
    for i, pid in enumerate(ids, 1):
        print(f"  {i:2d}. {pid}  â†’  https://map.naver.com/p/entry/place/{pid}")
